prism.ppt <- projectRaster(prism.ppt, crs=prjnlcd)
unlink(folderCreate, recursive = T)
ppt <- getValues(prism.ppt)
gridNum <- 1:length(ppt)
pptInfo <- data.frame(gridNum,ppt)
merged <- merge(gridInfo, pptInfo, by="gridNum", all.x=TRUE)
# NA control (If there is no data for the specific grid, use the average in the stco (or FIPS))
midx <- which(is.na(merged),arr.ind = T)[,1]
if (length(midx) != 0){
for (mi in 1:length(midx)){
stcomi <- merged[midx[mi],"stco"]
merged[midx[mi],"ppt"] <- mean(merged[which(merged[,"stco"]==stcomi),"ppt"],na.rm=T)
}
}
colnames(merged)[3:4] <- c('numAg', 'ppt')
out <- merged[,lapply(.SD, weighted.mean, w = numAg, na.rm = T),
by = list(stco), .SDcols = 'ppt']
merged <- as.data.table(merged)
out <- merged[,lapply(.SD, weighted.mean, w = numAg, na.rm = T),
by = list(stco), .SDcols = 'ppt']
dailyDataExtract <- function(i){
# import data
# cat(i, '\b')
# mon <- as.numeric(substr(yfilenames[i], 28, 29))
# cat(yfilenames[i], '\r')
# Unzip the file and extract the data, then delete the unzipped file
folderCreate <- paste0(tmpDir, '/temp_to_be_deleted_', i)
#cmd <- paste0('unzip -q ', zipDir, '/ppt/', m, '/', yfilenames[i],' -d', paste0(folderCreate))
cmd <- paste0('7z x -o', paste0(folderCreate), ' ', inputDir, '/ppt/', m, '/', yfilenames[i])
system(cmd)
prism.ppt <- raster(paste(folderCreate, sub('\\.zip$', '.bil', yfilenames[i]),sep = "/"))
prism.ppt <- projectRaster(prism.ppt, crs=prjnlcd)
unlink(folderCreate, recursive = T)
ppt <- getValues(prism.ppt)
gridNum <- 1:length(ppt)
pptInfo <- data.frame(gridNum,ppt)
merged <- merge(gridInfo, pptInfo, by="gridNum", all.x=TRUE)
# NA control (If there is no data for the specific grid, use the average in the stco (or FIPS))
midx <- which(is.na(merged),arr.ind = T)[,1]
if (length(midx) != 0){
for (mi in 1:length(midx)){
stcomi <- merged[midx[mi],"stco"]
merged[midx[mi],"ppt"] <- mean(merged[which(merged[,"stco"]==stcomi),"ppt"],na.rm=T)
}
}
colnames(merged)[3:4] <- c('numAg', 'ppt')
merged <- as.data.table(merged)
out <- merged[,lapply(.SD, weighted.mean, w = numAg, na.rm = T),
by = list(stco), .SDcols = 'ppt']
return(out)
}
dailyDataExtract(1)
rm(list=ls())
library(raster)
library(data.table)
library(rgdal)
library(parallel)
library(dplyr)
library(tidyr)
#------------------------------------------------------------------------------------------------------#
# Set the parameters prior to running the codes.
#------------------------------------------------------------------------------------------------------#
## NLCD projection
prjnlcd <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
## Specify the years of data to compile
years <- c(1981: 2019)
## countyfips-gridNumber bridge
gridInfoRaw <- read.table("../gridInfo.csv", header=TRUE, sep=",")
# Set the number of cores to use for parallel computing.
# detectCores() # check the number of cores
cores <- 8
# Specify the directory that saves the raw PRISM daily weather data.
inputDir <- 'C:/Users/bwchen/Box/gramig-lab-bowen/CRPEval/data/prism/daily'
# Specify the directory that saves the unzipped weather data.
tmpDir <- 'C:/Users/bwchen/Documents/'
# Specify the directory that saves the processed data (output)
outDir <- "C:/Users/bwchen/Box/gramig-lab-bowen/PRISM_DATA_PROCESS/"
# set the temperature interval
Tint <- c(-60:60) # Not have to change
# Define the growing season (such as from 0301 to 0831)
grow_season <- c('0801', '0831')
# Read the functions
source('precipitation_daily_extract_func.r')
m <- 2018
if(m <= 1995) selcol <- 3
if(m >= 1996 & m <= 2003) selcol <- 4
if(m >= 2004 & m <= 2008) selcol <- 5
if(m >= 2009) selcol <- 6
gridInfo <- gridInfoRaw [,c(1,2,selcol)]
selTime1 <- paste0(m, grow_season[1]); selTime2 <- paste0(m, grow_season[2]);
# List all the files.
yfilenames <- list.files(path = paste(inputDir, "/ppt",m,sep = '/'))
# Number of zipped files contained in one folder. Since one zipped file has data for one day
# this number is also the number of days in the year, which shall be equal to 365
ni <- length(yfilenames) # Return the number of files in the folder.
if(ni < 365) {stop; print('Number of days in the year is smalled than 365')}
# Extract all the daily data using parallel computing
# dailyData <- mclapply(1:ni, dailyDataExtract, mc.cores = 4)
## Get the data in the growing season only, rather than extracting all the daily data
# Extract the data during the growing season only.
cl <- makeCluster(cores)
clusterExport(cl, c("m", 'yfilenames', 'prjnlcd', 'gridInfo', 'nT', 'inputDir', 'tmpDir'))
clusterEvalQ(cl, library(raster))
clusterEvalQ(cl, library(rgdal))
clusterEvalQ(cl, library(dplyr))
clusterEvalQ(cl, library(data.table))
clusterExport(cl, c("m", 'yfilenames', 'prjnlcd', 'gridInfo', 'inputDir', 'tmpDir'))
dailyData <- parLapply(cl, grep(selTime1, yfilenames):grep(selTime2, yfilenames), dailyDataExtract)
# dailyData <- mclapply(grep(selTime1, yfilenames):grep(selTime2, yfilenames),
#                        dailyDataExtract, mc.cores = 4) # mac version
stopCluster(cl)
countyAggDataWide <- bind_rows(dailyData)
countyAggDataWide <- bind_rows(dailyData) %>%
group_by(stco) %>%
summarise(ppt = sum(ppt))
View(countyAggDataWide)
countyAggDataWide <- bind_rows(dailyData) %>%
group_by(stco) %>%
summarise(ppt = sum(ppt)) %>%
mutate(year = m)
countySavePath <- paste0(outDir, selTime1, '_', selTime2, '_', 'pptCounty', m, '.csv')
write.csv(x = countyAggDataWide, file = countySavePath, row.names = F)
rm(list=ls())
library(raster)
library(data.table)
library(rgdal)
library(parallel)
library(dplyr)
library(tidyr)
#------------------------------------------------------------------------------------------------------#
# Set the parameters prior to running the codes.
#------------------------------------------------------------------------------------------------------#
## NLCD projection
prjnlcd <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
## Specify the years of data to compile
years <- c(1981: 2019)
## countyfips-gridNumber bridge
gridInfoRaw <- read.table("../gridInfo.csv", header=TRUE, sep=",")
# Set the number of cores to use for parallel computing.
# detectCores() # check the number of cores
cores <- 8
# Specify the directory that saves the raw PRISM daily weather data.
inputDir <- 'C:/Users/bwchen/Box/gramig-lab-bowen/CRPEval/data/prism/daily'
# Specify the directory that saves the unzipped weather data.
tmpDir <- 'C:/Users/bwchen/Documents/'
# Specify the directory that saves the processed data (output)
outDir <- "C:/Users/bwchen/Box/gramig-lab-bowen/PRISM_DATA_PROCESS/"
# Define the growing season (such as from 0301 to 0831)
grow_season <- c('0301', '0831')
# Read the functions
source('precipitation_daily_extract_func.r')
#------------------------------------------------------------------------------------------------------#
# Now running the codes.
#------------------------------------------------------------------------------------------------------#
for(m in years){
cat('Working on data for', m, '\n')
## years for each bridge
# (col3) 1992: 1981-1995 [make selcol be 3]
# (col4) 2001: 1996-2003 [make selcol be 4]
# (col5) 2006: 2004-2008 [make selcol be 5]
# (col6) 2011: 2009-2018 (or later) [make selcol be 6]
if(m <= 1995) selcol <- 3
if(m >= 1996 & m <= 2003) selcol <- 4
if(m >= 2004 & m <= 2008) selcol <- 5
if(m >= 2009) selcol <- 6
gridInfo <- gridInfoRaw [,c(1,2,selcol)]
selTime1 <- paste0(m, grow_season[1]); selTime2 <- paste0(m, grow_season[2]);
# List all the files.
yfilenames <- list.files(path = paste(inputDir, "/ppt",m,sep = '/'))
# Number of zipped files contained in one folder. Since one zipped file has data for one day
# this number is also the number of days in the year, which shall be equal to 365
ni <- length(yfilenames) # Return the number of files in the folder.
if(ni < 365) {stop; print('Number of days in the year is smalled than 365')}
# Extract all the daily data using parallel computing
# dailyData <- mclapply(1:ni, dailyDataExtract, mc.cores = 4)
## Get the data in the growing season only, rather than extracting all the daily data
# Extract the data during the growing season only.
cl <- makeCluster(cores)
clusterExport(cl, c("m", 'yfilenames', 'prjnlcd', 'gridInfo', 'inputDir', 'tmpDir'))
clusterEvalQ(cl, library(raster))
clusterEvalQ(cl, library(rgdal))
clusterEvalQ(cl, library(dplyr))
clusterEvalQ(cl, library(data.table))
dailyData <- parLapply(cl, grep(selTime1, yfilenames):grep(selTime2, yfilenames), dailyDataExtract)
# dailyData <- mclapply(grep(selTime1, yfilenames):grep(selTime2, yfilenames),
#                        dailyDataExtract, mc.cores = 4) # mac version
stopCluster(cl)
# Return the data in wide format: add gdds arcoss growing season.
Ncol <- ncol(dailyData[[1]])
countyAggDataWide <- bind_rows(dailyData) %>%
group_by(stco) %>%
summarise(ppt = sum(ppt)) %>%
mutate(year = m)
countySavePath <- paste0(outDir, selTime1, '_', selTime2, '_', 'pptCounty', m, '.csv')
write.csv(x = countyAggDataWide, file = countySavePath, row.names = F)
# Remove the processed data after saving them into the local directory
# rm(dailyData, countyAggDataWide, cl)
}
m <- 2018
cat('Working on data for', m, '\n')
## years for each bridge
# (col3) 1992: 1981-1995 [make selcol be 3]
# (col4) 2001: 1996-2003 [make selcol be 4]
# (col5) 2006: 2004-2008 [make selcol be 5]
# (col6) 2011: 2009-2018 (or later) [make selcol be 6]
if(m <= 1995) selcol <- 3
if(m >= 1996 & m <= 2003) selcol <- 4
if(m >= 2004 & m <= 2008) selcol <- 5
if(m >= 2009) selcol <- 6
gridInfo <- gridInfoRaw [,c(1,2,selcol)]
selTime1 <- paste0(m, grow_season[1]); selTime2 <- paste0(m, grow_season[2]);
# List all the files.
yfilenames <- list.files(path = paste(inputDir, "/ppt",m,sep = '/'))
# Number of zipped files contained in one folder. Since one zipped file has data for one day
# this number is also the number of days in the year, which shall be equal to 365
ni <- length(yfilenames) # Return the number of files in the folder.
if(ni < 365) {stop; print('Number of days in the year is smalled than 365')}
# Extract all the daily data using parallel computing
# dailyData <- mclapply(1:ni, dailyDataExtract, mc.cores = 4)
## Get the data in the growing season only, rather than extracting all the daily data
# Extract the data during the growing season only.
cl <- makeCluster(cores)
clusterExport(cl, c("m", 'yfilenames', 'prjnlcd', 'gridInfo', 'inputDir', 'tmpDir'))
clusterEvalQ(cl, library(raster))
clusterEvalQ(cl, library(rgdal))
clusterEvalQ(cl, library(dplyr))
clusterEvalQ(cl, library(data.table))
dailyData <- parLapply(cl, grep(selTime1, yfilenames):grep(selTime2, yfilenames), dailyDataExtract)
# dailyData <- mclapply(grep(selTime1, yfilenames):grep(selTime2, yfilenames),
#                        dailyDataExtract, mc.cores = 4) # mac version
stopCluster(cl)
# Return the data in wide format: add gdds arcoss growing season.
Ncol <- ncol(dailyData[[1]])
countyAggDataWide <- bind_rows(dailyData) %>%
group_by(stco) %>%
summarise(ppt = sum(ppt)) %>%
mutate(year = m)
countySavePath <- paste0(outDir, selTime1, '_', selTime2, '_', 'pptCounty', m, '.csv')
write.csv(x = countyAggDataWide, file = countySavePath, row.names = F)
rm(list=ls())
library(raster)
library(data.table)
library(rgdal)
library(parallel)
library(dplyr)
library(tidyr)
#------------------------------------------------------------------------------------------------------#
# Set the parameters prior to running the codes.
#------------------------------------------------------------------------------------------------------#
## NLCD projection
prjnlcd <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
## Specify the years of data to compile
years <- c(1981: 2019)
## countyfips-gridNumber bridge
gridInfoRaw <- read.table("../gridInfo.csv", header=TRUE, sep=",")
# Set the number of cores to use for parallel computing.
# detectCores() # check the number of cores
cores <- 8
# Specify the directory that saves the raw PRISM daily weather data.
inputDir <- 'C:/Users/bwchen/Box/gramig-lab-bowen/CRPEval/data/prism/daily'
# Specify the directory that saves the unzipped weather data.
tmpDir <- 'C:/Users/bwchen/Documents/'
# Specify the directory that saves the processed data (output)
outDir <- "C:/Users/bwchen/Box/gramig-lab-bowen/PRISM_DATA_PROCESS/"
# set the temperature interval
Tint <- c(-60:60) # Not have to change
nT <- length(Tint)
# Define the growing season (such as from 0301 to 0831)
grow_season <- c('0301', '0831')
# Define the growing season (such as from 0301 to 0831)
grow_season <- c('0820', '0831')
# Read the functions
source('temperature_daily_extract_func.r')
m <- 2018
cat('Working on data for', m, '\n')
if(m <= 1995) selcol <- 3
if(m >= 1996 & m <= 2003) selcol <- 4
if(m >= 2004 & m <= 2008) selcol <- 5
if(m >= 2009) selcol <- 6
gridInfo <- gridInfoRaw [,c(1,2,selcol)]
selTime1 <- paste0(m, grow_season[1]); selTime2 <- paste0(m, grow_season[2]);
# List all the files.
yfilenames <- list.files(path = paste(inputDir, "/ppt",m,sep = '/'))
# List all the files.
yfilenames.max <- list.files(path = paste(inputDir, "/tmax",m,sep = '/'))
yfilenames.min <- list.files(path = paste(inputDir, "/tmin",m,sep = '/'))
# Number of zipped files contained in one folder. Since one zipped file has data for one day
# this number is also the number of days in the year, which shall be equal to 365
ni <- length(yfilenames.max) # Return the number of files in the folder.
if(ni < 365) {stop; print('Number of days in the year is smalled than 365')}
# Extract all the daily data using parallel computing
# dailyData <- mclapply(1:ni, dailyDataExtract, mc.cores = 4)
## Get the data in the growing season only, rather than extracting all the daily data
# Extract the data during the growing season only.
cl <- makeCluster(cores)
clusterExport(cl, c("m", 'yfilenames', 'prjnlcd', 'gridInfo', 'nT', 'days.in.range', 'Tint', 'inputDir', 'tmpDir'))
clusterEvalQ(cl, library(raster))
clusterEvalQ(cl, library(rgdal))
clusterEvalQ(cl, library(dplyr))
clusterEvalQ(cl, library(data.table))
dailyData <- parLapply(cl, grep(selTime1, yfilenames):grep(selTime2, yfilenames), dailyDataExtract)
# Extract all the daily data using parallel computing
# dailyData <- mclapply(1:ni, dailyDataExtract, mc.cores = 4)
## Get the data in the growing season only, rather than extracting all the daily data
# Extract the data during the growing season only.
cl <- makeCluster(cores)
clusterExport(cl, c("m", 'yfilenames.max', 'yfilenames.min', 'prjnlcd', 'gridInfo', 'nT', 'days.in.range', 'Tint', 'inputDir', 'tmpDir'))
clusterEvalQ(cl, library(raster))
clusterEvalQ(cl, library(rgdal))
clusterEvalQ(cl, library(dplyr))
clusterEvalQ(cl, library(data.table))
dailyData <- parLapply(cl, grep(selTime1, yfilenames):grep(selTime2, yfilenames), dailyDataExtract)
# dailyData <- mclapply(grep(selTime1, yfilenames):grep(selTime2, yfilenames),
#                        dailyDataExtract, mc.cores = 4) # mac version
stopCluster(cl)
aggData <- dailyData %>% bind_rows()
warnings()
aggData <- dailyData %>%
bind_rows() %>%
group_by(stco) %>%
summarise_all(sum)
aggData <- dailyData %>%
bind_rows() %>%
group_by(stco) %>%
summarise_all(sum) %>%
mutate(year = m)
countySavePath <- paste0(outDir, selTime1, '_', selTime2, '_', 'pptCounty', m, '.csv')
check <- fread('./20180820_20180831_gddCounty2018.csv')
check <- fread('20180820_20180831_gddCounty2018.csv')
getwd()
check <- fread('./20180820_20180831_gddCounty2018.csv')
check <- fread('../20180820_20180831_gddCounty2018.csv')
all.equal(aggData[,-123], check[,-1])
View(aggData)
countySavePath <- paste0(outDir, selTime1, '_', selTime2, '_', 'gddCounty2', m, '.csv')
write.csv(x = aggData, file = countySavePath, row.names = F)
rm(list=ls())
library(raster)
library(data.table)
library(rgdal)
library(parallel)
library(dplyr)
library(tidyr)
#------------------------------------------------------------------------------------------------------#
# Set the parameters prior to running the codes.
#------------------------------------------------------------------------------------------------------#
## NLCD projection
prjnlcd <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
## Specify the years of data to compile
years <- c(1981: 2019)
## countyfips-gridNumber bridge
gridInfoRaw <- read.table("../gridInfo.csv", header=TRUE, sep=",")
# Set the number of cores to use for parallel computing.
# detectCores() # check the number of cores
cores <- 8
# Specify the directory that saves the raw PRISM daily weather data.
inputDir <- 'C:/Users/bwchen/Box/gramig-lab-bowen/CRPEval/data/prism/daily'
# Specify the directory that saves the unzipped weather data.
tmpDir <- 'C:/Users/bwchen/Documents/'
# Specify the directory that saves the processed data (output)
outDir <- "C:/Users/bwchen/Box/gramig-lab-bowen/PRISM_DATA_PROCESS/"
# Define the growing season (such as from 0301 to 0831)
grow_season <- c('0301', '0831')
# Read the functions
source('precipitation_daily_extract_func.r')
m <- 2018
i <- 1
data <- dailyDataExtract(60)
if(m <= 1995) selcol <- 3
if(m >= 1996 & m <= 2003) selcol <- 4
if(m >= 2004 & m <= 2008) selcol <- 5
if(m >= 2009) selcol <- 6
gridInfo <- gridInfoRaw [,c(1,2,selcol)]
selTime1 <- paste0(m, grow_season[1]); selTime2 <- paste0(m, grow_season[2]);
# List all the files.
yfilenames <- list.files(path = paste(inputDir, "/ppt",m,sep = '/'))
# Number of zipped files contained in one folder. Since one zipped file has data for one day
# this number is also the number of days in the year, which shall be equal to 365
ni <- length(yfilenames) # Return the number of files in the folder.
if(ni < 365) {stop; print('Number of days in the year is smalled than 365')}
data <- dailyDataExtract(60)
rm(list=ls())
library(raster)
library(data.table)
library(rgdal)
library(parallel)
library(dplyr)
library(tidyr)
#------------------------------------------------------------------------------------------------------#
# Set the parameters prior to running the codes.
#------------------------------------------------------------------------------------------------------#
## NLCD projection
prjnlcd <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
## Specify the years of data to compile
years <- c(1981: 2019)
## countyfips-gridNumber bridge
gridInfoRaw <- read.table("../gridInfo.csv", header=TRUE, sep=",")
# Set the number of cores to use for parallel computing.
# detectCores() # check the number of cores
cores <- 8
# Specify the directory that saves the raw PRISM daily weather data.
inputDir <- 'C:/Users/bwchen/Box/gramig-lab-bowen/CRPEval/data/prism/daily'
# Specify the directory that saves the unzipped weather data.
tmpDir <- 'C:/Users/bwchen/Documents/'
# Specify the directory that saves the processed data (output)
outDir <- "C:/Users/bwchen/Box/gramig-lab-bowen/PRISM_DATA_PROCESS/"
# Define the growing season (such as from 0301 to 0831)
grow_season <- c('0821', '0831')
# Read the functions
source('precipitation_daily_extract_func.r')
m <- 2018
cat('Working on data for', m, '\n')
if(m <= 1995) selcol <- 3
if(m >= 1996 & m <= 2003) selcol <- 4
if(m >= 2004 & m <= 2008) selcol <- 5
if(m >= 2009) selcol <- 6
gridInfo <- gridInfoRaw [,c(1,2,selcol)]
selTime1 <- paste0(m, grow_season[1]); selTime2 <- paste0(m, grow_season[2]);
# List all the files.
yfilenames <- list.files(path = paste(inputDir, "/ppt",m,sep = '/'))
# Number of zipped files contained in one folder. Since one zipped file has data for one day
# this number is also the number of days in the year, which shall be equal to 365
ni <- length(yfilenames) # Return the number of files in the folder.
if(ni < 365) {stop; print('Number of days in the year is smalled than 365')}
# Extract all the daily data using parallel computing
# dailyData <- mclapply(1:ni, dailyDataExtract, mc.cores = 4)
## Get the data in the growing season only, rather than extracting all the daily data
# Extract the data during the growing season only.
cl <- makeCluster(cores)
clusterExport(cl, c("m", 'yfilenames', 'prjnlcd', 'gridInfo', 'inputDir', 'tmpDir'))
clusterEvalQ(cl, library(raster))
clusterEvalQ(cl, library(rgdal))
clusterEvalQ(cl, library(dplyr))
clusterEvalQ(cl, library(data.table))
dailyData <- parLapply(cl, grep(selTime1, yfilenames):grep(selTime2, yfilenames), dailyDataExtract)
# dailyData <- mclapply(grep(selTime1, yfilenames):grep(selTime2, yfilenames),
#                        dailyDataExtract, mc.cores = 4) # mac version
stopCluster(cl)
# Return the data in wide format: add gdds arcoss growing season.
dataAgg <- bind_rows(dailyData) %>%
group_by(stco) %>%
summarise(ppt = sum(ppt)) %>%
mutate(year = m)
rm(list=ls())
library(raster)
library(data.table)
library(rgdal)
library(parallel)
library(dplyr)
library(tidyr)
#------------------------------------------------------------------------------------------------------#
# Set the parameters prior to running the codes.
#------------------------------------------------------------------------------------------------------#
## NLCD projection
prjnlcd <- "+proj=aea +lat_1=29.5 +lat_2=45.5 +lat_0=23 +lon_0=-96 +x_0=0 +y_0=0 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs"
## Specify the years of data to compile
years <- c(1981: 2019)
## countyfips-gridNumber bridge
gridInfoRaw <- read.table("../gridInfo.csv", header=TRUE, sep=",")
# Set the number of cores to use for parallel computing.
# detectCores() # check the number of cores
cores <- 8
# Specify the directory that saves the raw PRISM daily weather data.
inputDir <- 'C:/Users/bwchen/Box/gramig-lab-bowen/CRPEval/data/prism/daily'
# Specify the directory that saves the unzipped weather data.
tmpDir <- 'C:/Users/bwchen/Documents/'
# Specify the directory that saves the processed data (output)
outDir <- "C:/Users/bwchen/Box/gramig-lab-bowen/PRISM_DATA_PROCESS/"
# Define the growing season (such as from 0301 to 0831)
grow_season <- c('0821', '0831')
# Read the functions
source('precipitation_daily_extract_func.r')
m <- 2018
# Define the growing season (such as from 0301 to 0831)
grow_season <- c('0301', '0831')
# Read the functions
source('precipitation_daily_extract_func.r')
m <- 2018
cat('Working on data for', m, '\n')
if(m <= 1995) selcol <- 3
if(m >= 1996 & m <= 2003) selcol <- 4
if(m >= 2004 & m <= 2008) selcol <- 5
if(m >= 2009) selcol <- 6
gridInfo <- gridInfoRaw [,c(1,2,selcol)]
selTime1 <- paste0(m, grow_season[1]); selTime2 <- paste0(m, grow_season[2]);
# List all the files.
yfilenames <- list.files(path = paste(inputDir, "/ppt",m,sep = '/'))
# Number of zipped files contained in one folder. Since one zipped file has data for one day
# this number is also the number of days in the year, which shall be equal to 365
ni <- length(yfilenames) # Return the number of files in the folder.
if(ni < 365) {stop; print('Number of days in the year is smalled than 365')}
# Extract all the daily data using parallel computing
# dailyData <- mclapply(1:ni, dailyDataExtract, mc.cores = 4)
## Get the data in the growing season only, rather than extracting all the daily data
# Extract the data during the growing season only.
cl <- makeCluster(cores)
clusterExport(cl, c("m", 'yfilenames', 'prjnlcd', 'gridInfo', 'inputDir', 'tmpDir'))
clusterEvalQ(cl, library(raster))
clusterEvalQ(cl, library(rgdal))
clusterEvalQ(cl, library(dplyr))
clusterEvalQ(cl, library(data.table))
dailyData <- parLapply(cl, grep(selTime1, yfilenames):grep(selTime2, yfilenames), dailyDataExtract)
# dailyData <- mclapply(grep(selTime1, yfilenames):grep(selTime2, yfilenames),
#                        dailyDataExtract, mc.cores = 4) # mac version
stopCluster(cl)
# Return the data in wide format: add gdds arcoss growing season.
dataAgg <- bind_rows(dailyData) %>%
group_by(stco) %>%
summarise(ppt = sum(ppt)) %>%
mutate(year = m)
countySavePath <- paste0(outDir, selTime1, '_', selTime2, '_', 'pptCounty', m, '.csv')
write.csv(x = dataAgg, file = countySavePath, row.names = F)
